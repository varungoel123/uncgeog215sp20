---
title: "Download tweets using Twitter API"
author: 
- GEOG215, Introduction to Spatial Data Science \newline
- Varun Goel
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
   tufte::tufte_html: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This document contains a walkthrough of the steps needed to use Twitter's API in R to search Twitter using keywords, download tweets, and do some simple processing and analysis.

# Twitter account

[Sign up for a Twitter account](https://twitter.com)  

**Send me your twitter handle/username either as a private message on piazza or on email**. You will shortly receive a message on your registered email to access the Twitter developer account.

[Navigate to the Twitter developer account](https://developer.twitter.com/), and login with your credentials.

******
 
# Create a new App on Twitter

Follow the instructions here at<https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html>.  
For mine, I used the following:  
- *App name:* geog215-coronavirus 

- *Application description:* This is an app that I need to create for a class I am teaching I can access tweets via R  

- *Website URL:* https://geog215-spds.rbind.io

- *Tell us how this app will be used*: (Write 100 characters or more) You can mention that in your own words that you are using it for the GEOG 215 class to learn how to mine tweets via twitter, lean them, analyze them and map them for spatial and social analysis.
******
 
# Access tweets from R 

You will need the following packages to run the code in this document. Install them if you have not already done so. The `rtweet` package is the one that allows you to access the Twitter API. The others are used for processing and displaying the data.

```{r, message=FALSE}
library(rtweet)
library(maps)
library(tidytext)
library(dplyr) # this should already be installed
library(stringr)
library(wordcloud2)
library(ggplot2) # this should already be installed 2
library(sf)
```

******
 
## Get token information

[Follow the instructions here](https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html) to retrieve the access tokens for Twitter. I am using the second approach provided at this source, which **does not require interacting with twitter via a web browser to authenticate**. I recommend you use this one too] This function is basically what lets your computer (and R) to communicate with Twitter's servers.

Use the following command, but substitute the information from your App/key/token/secret.

```{r include=FALSE}
token <- create_token(
  app = "geog215-coronavirus",
  consumer_key = "4UJa8OYqSNwyZqfTtNMNQj03D",
  consumer_secret = "kxI3PCwm08aVPp9rnYhrKs3TshNn6GcafmE7jo0UdwTjjiEG1X",
  access_token = "727055786-rU9vSEQsmWT3npnNWurU1Cf48zwaphDTa04d9n0U",
  access_secret = "sun9nRG4e0CZZ4grQkZlIvZ3w2HscrwpHezUMgzWxDZJC")
```


```{r eval=FALSE}
token <- create_token(
  app = "Your app name",
  consumer_key = "your consumer api key",
  consumer_secret = "your consumer api secret",
  access_token = "your access token",
  access_secret = "your access secret")
```

******
 
The next few commands search tweets in english for the words "coronavirus" or "ncov" or "2019-ncov" (and other variants of these words), retrieves up to the first 5,000^[the limit that can be returned in a single query is 18,000. Twitter limits 18000 queries for every 15 mins, so i suggest retrieving only 5000 the first time]) tweets that are not retweets, and then provides summary information about the data.

```{r}
# Get tweets
ncov_tw <- search_tweets(q = "coronavirus OR ncov OR 2019-ncov", n = 5000, include_rts = FALSE, lang = "en")
```

******

However this raw twitter data is really messy and needs to be cleaned for unneeded punctuations and words. Use the following code to get rid of some of the unnecessary words.

```{r}
ncov_tw <- ncov_tw %>%
  mutate(stripped_text = text %>% gsub("http.*","",.)) %>%
  mutate(stripped_text = stripped_text %>% gsub("https.*","",.)) %>%
  mutate(stripped_text = stripped_text %>% gsub("\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)", 
       "", .))

# Unnest the words from the tweets
hmtTable <-  unnest_tokens(ncov_tw, word, text)

# Compare
ncov_tw$text[1:5]
hmtTable$word[1:5]

## Remove stop words - very common words such as "the", "of", etc
# Load the data of stop words
data(stop_words)
# anti_join is basically a "remove" command
hmtTable <-  anti_join(hmtTable, stop_words, by="word")
```

Now lets plot an ordered graph of the most commonly used words.

```{r}
# plot the top 30 words -- notice any issues?
hmtTable %>%
  count(word, sort = TRUE) %>%
  top_n(30) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Count",
       x = "Unique words",
       title = "Count of unique words found in tweets",
       subtitle = "Stop words removed from the list")
```

I find that there are still lots of unnecessary words. So i remove them and plot them again.

```{r}
hmtTable <- filter(hmtTable, !word %in% c('t.co', 'https', 'handmaidstale', "handmaid's", 'season', 'episode', 'de', 'handmaidsonhulu',  'tvtime', 'watched', 'watching', 'watch', 'la', "it's", 'el', 'en', 'tv', 'je', 'ep', 'week', 'amp'))

# plot the top 30 words -- notice any issues?
hmtTable %>%
  count(word, sort = TRUE) %>%
  top_n(30) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(y = "Count",
       x = "Unique words",
       title = "Count of unique words found in tweets",
       subtitle = "Stop words removed from the list")
```

The following code puts the data into a format that can be used to create a word cloud. Most of this code came from [this source](https://www.r-bloggers.com/awesome-twitter-word-clouds-in-r/).

```{r}

# Convert to a table of frequencies
hmtTable <-  count(hmtTable, word, sort = TRUE) %>%
  filter(n>100)

# Create word cloud with red text
wordcloud2(hmtTable, size=1)
```

We can remove the common keywords to look at some uncommon ones


```{r}
hmtTable <- hmtTable %>% filter(!word %in% c("coronavirus","ncov",
                                             "china","virus","wuhan"))
wordcloud2(hmtTable, size=1)
```

The following code creates a nice time-series plot of the number of tweets over the past seven days (this is the limit for the Twitter API). I dont get anything meaningful because Coronavirus is really popular as a topic and the 5000 tweets got exhausted going back a few hours only. The code to create the plot came from [this source](https://rtweet.info/).

```{r}
ncov_tw %>%
   ts_plot("mins") +
   ggplot2::theme_minimal() +
   ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
   ggplot2::labs(
     x = NULL, y = NULL,
     title = "Frequency of tweets with words related to coronavirus from past 7 days",
     subtitle = "Tweets aggregated using minute intervals",
     caption = "\nSource: Data collected from Twitter's REST API via rtweet"
   )
```

The following code creates a decent looking map of where the tweets originated. The code to create the plot came from [this source](https://eriqande.github.io/rep-res-web/lectures/making-maps-with-R.html).


```{r, message=FALSE}
## Create lat/lng variables using all available tweet and profile geo-location data
geo.ncov_tw <- lat_lng(ncov_tw)
```

We can actually create our own vector data now by taking the data frame `geo.ncov_tw` and converting it into an sf object

```{r, message=FALSE}
## Create lat/lng variables using all available tweet and profile geo-location data
tweets_sf <- geo.ncov_tw %>%
  filter(!is.na(lng)) %>%
  st_as_sf(coords = c("lng","lat"),
                      crs = 4326)
print(tweets_sf[92], n = 3)
```

But for now, we settle using other rudimentary packages and settle with a not so useful map

```{r}
## Get state boundaries
countries <- map_data("world")

## Subset your country
my_country<- countries %>%
  filter(region == "USA")

## Create base map
st_base <- ggplot(data = my_country, mapping = aes(x = long, y = lat, group = group)) + 
  coord_quickmap() + 
  geom_polygon(color = "black", fill = "gray")

## Add tweets with geographic coordinates
st_base + 
  geom_point(data = geo.ncov_tw[which(!is.na(geo.ncov_tw$lng)),], aes(lng,lat,fill=NULL,group=NULL), col = rgb(0, .3, .7, .75), size=1) +
 coord_quickmap(xlim = c(-125,-66),  ylim = c(24,50))
```

 You can find your Country choice bounding box (xlim, ylim ) at
 <https://gist.github.com/graydon/11198540>
